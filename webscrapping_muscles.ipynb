{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://exrx.net/Lists/Directory\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all divs with class \"col-sm-6\"\n",
    "exercise_divs = soup.find_all(\"div\", class_=\"col-sm-6\")\n",
    "\n",
    "# Extracting the a href names and their subsections\n",
    "exercise_data = []\n",
    "\n",
    "for div in exercise_divs:\n",
    "    links = div.find_all(\"a\")  # Find all <a> tags inside the div\n",
    "    for link in links:\n",
    "        name = link.text.strip()  # Extract the name\n",
    "        href = link[\"href\"] if \"href\" in link.attrs else None  # Extract the link\n",
    "        full_url = url + href if href else None  # Convert relative links to absolute\n",
    "        exercise_data.append((name, full_url))\n",
    "\n",
    "# Initialize variables\n",
    "data = []\n",
    "\n",
    "# Print the extracted data\n",
    "main = \"\"\n",
    "for name, link in exercise_data:\n",
    "    path_parts = link.split(\"#\")[-1:]  # Get the last part\n",
    "    if(path_parts[0] == link):\n",
    "        #print(f\"Exercise: {name}\")\n",
    "        main = name\n",
    "    else:\n",
    "        #print(f\"Exercise: {main} - Connected: {path_parts[0]}\")\n",
    "        data.append((main, path_parts[0]))  # Append main muscle and specific area\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Main Muscle\", \"Specific Area\"])\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "#file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/muscle_data.csv\"\n",
    "#df.to_csv(file_path, index=False)\n",
    "\n",
    "#print(f\"File saved: {file_path}\")\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "List = ['https://exrx.net/Lists/ExList/NeckWt','https://exrx.net/Lists/ExList/ShouldWt','https://exrx.net/Lists/ExList/ArmWt','https://exrx.net/Lists/ExList/ForeArmWt','https://exrx.net/Lists/ExList/BackWt','https://exrx.net/Lists/ExList/ChestWt','https://exrx.net/Lists/ExList/WaistWt','https://exrx.net/Lists/ExList/HipsWt','https://exrx.net/Lists/ExList/ThighWt','https://exrx.net/Lists/ExList/CalfWt']",
   "id": "73ee77456e4ab5f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://exrx.net/Lists/ExList/NeckWt\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all divs with class \"col-sm-6\"\n",
    "exercise_divs = soup.find_all(\"div\", class_=[\"col-sm-6\",\"col-sm-12\"])\n",
    "\n",
    "# Extracting the a href names and their subsections\n",
    "exercise_data = []\n",
    "\n",
    "for div in exercise_divs:\n",
    "    links = div.find_all(\"a\")  # Find all <a> tags inside the div\n",
    "    for link in links:\n",
    "        name = link.text.strip()  # Extract the name\n",
    "        href = link[\"href\"] if \"href\" in link.attrs else None  # Extract the link\n",
    "        if href:\n",
    "            full_url = urllib.parse.urljoin(url, href)  # Convert relative links to absolute\n",
    "            cleaned_link = full_url.replace(\"/Lists/ExList/NeckWt../..\", \"\")  # Remove the unnecessary part\n",
    "        else:\n",
    "            full_url = None\n",
    "            cleaned_link = None\n",
    "\n",
    "        exercise_data.append((name, cleaned_link))\n",
    "\n",
    "# Print the extracted data\n",
    "list_exercises = []\n",
    "for name, link in exercise_data:\n",
    "    if(link!=None):\n",
    "        # Extract only the last two parts of the path\n",
    "        path_parts = link.split(\"/\")[-2:]  # Get the last two parts\n",
    "        short_link = \"/\".join(path_parts)  # Join them back into a string\n",
    "        print(f\"Exercise: {name} - Link: {link}\")\n",
    "        list_exercises.append(link)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ],
   "id": "9a5833aed85418bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the target website\n",
    "#url = \"https://exrx.net/WeightExercises/GluteusMaximus/SLSquat\"\n",
    "url = \"https://exrx.net/WeightExercises/Sternocleidomastoid/LVLateralNeckFlexionH\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Ensure the driver quits after loading the page\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# Locate the string 'Utility', find the next <a> tag, and extract its text\n",
    "def extract_utility_name(soup,value):\n",
    "    \"\"\"\n",
    "    Finds the string 'Utility' in a <strong> tag, gets the next <a> tag,\n",
    "    and extracts its text.\n",
    "    \"\"\"\n",
    "    element = soup.find('strong', string=value)\n",
    "    if element:\n",
    "        next_a = element.find_next('a')  # Find the next <a> tag\n",
    "        if next_a:\n",
    "            return next_a.text.strip()  # Extract the text inside the <a> tag\n",
    "    return None  # Return None if not found\n",
    "\n",
    "\n",
    "# Call the function and fetch the utility name\n",
    "utility = extract_utility_name(soup, 'Utility:')\n",
    "mechanics = extract_utility_name(soup, 'Mechanics:')\n",
    "force = extract_utility_name(soup, 'Force:')\n",
    "preparation = soup.find('strong', string='Preparation').find_next('p').string.strip()\n",
    "execution = soup.find('strong', string='Execution').find_next('p').string.strip()\n",
    "\n",
    "# Extract muscles information\n",
    "target = [li.string.strip() for li in soup.find('strong', string='Target').find_next('ul').find_all('li')]\n",
    "print(target)\n",
    "synergists = [li.string.strip() for li in soup.find('strong', string='Synergists').find_next('ul').find_all('li')]\n",
    "print(synergists)\n",
    "dynamic_stabilizers = [li.string.strip() for li in soup.find('strong', string=lambda x: x in ['Dynamic Stabilizers', 'Stabilizers']).find_next('ul').find_all('li')]\n",
    "\n",
    "print(dynamic_stabilizers)\n",
    "\n",
    "\n",
    "# Create pandas DataFrame\n",
    "data = {\n",
    "    'Utility': [utility],\n",
    "    'Mechanics': [mechanics],\n",
    "    'Force': [force],\n",
    "    'Preparation': [preparation],\n",
    "    'Execution': [execution],\n",
    "    'Target': [target],\n",
    "    'Synergists': [synergists],\n",
    "    'Dynamic Stabilizers': [dynamic_stabilizers]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/exercise_solo_data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n"
   ],
   "id": "a437517e8a066fce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# List of URLs to scrape\n",
    "url_list = [\n",
    "    \"https://exrx.net/WeightExercises/Sternocleidomastoid/CBNeckFlx\",\n",
    "    \"https://exrx.net/WeightExercises/PectoralSternal/BBBenchPress\",\n",
    "    \"https://exrx.net/WeightExercises/RectusAbdominis/BWCrunch\",\n",
    "    \"https://exrx.net/WeightExercises/GluteusMaximus/SLSquat\"\n",
    "]\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "# Function to extract utility data\n",
    "def extract_utility_name(soup, value):\n",
    "    \"\"\"\n",
    "    Finds the string in a <strong> tag, gets the next <a> tag,\n",
    "    and extracts its text.\n",
    "    \"\"\"\n",
    "    element = soup.find('strong', string=value)\n",
    "    if element:\n",
    "        next_a = element.find_next('a')  # Find the next <a> tag\n",
    "        if next_a:\n",
    "            return next_a.text.strip()  # Extract the text inside the <a> tag\n",
    "    return None  # Return None if not found\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all rows of data\n",
    "all_exercise_data = []\n",
    "\n",
    "# Iterate through URLs in the list\n",
    "for url in list_exercises:\n",
    "    try:\n",
    "        # Navigate to the target website\n",
    "        driver.get(url)\n",
    "\n",
    "        # Get page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract exercise details\n",
    "        utility = extract_utility_name(soup, 'Utility:')\n",
    "        mechanics = extract_utility_name(soup, 'Mechanics:')\n",
    "        force = extract_utility_name(soup, 'Force:')\n",
    "\n",
    "        # Extract preparation and execution details\n",
    "        preparation = soup.find('strong', string='Preparation').find_next('p').string.strip()\n",
    "        execution = soup.find('strong', string='Execution').find_next('p').string.strip()\n",
    "\n",
    "        # Extract muscles information\n",
    "        target = [li.string.strip() for li in soup.find('strong', string='Target').find_next('ul').find_all('li')]\n",
    "        synergists = [li.string.strip() for li in\n",
    "                      soup.find('strong', string='Synergists').find_next('ul').find_all('li')]\n",
    "        dynamic_stabilizers = [\n",
    "            li.string.strip()\n",
    "            for li in soup.find('strong', string=lambda x: x in ['Dynamic Stabilizers', 'Stabilizers'])\n",
    "            .find_next('ul').find_all('li')\n",
    "        ]\n",
    "\n",
    "        # Add the extracted data to the list\n",
    "        all_exercise_data.append({\n",
    "            'URL': url,\n",
    "            'Utility': utility,\n",
    "            'Mechanics': mechanics,\n",
    "            'Force': force,\n",
    "            'Preparation': preparation,\n",
    "            'Execution': execution,\n",
    "            'Target': target,\n",
    "            'Synergists': synergists,\n",
    "            'Dynamic Stabilizers': dynamic_stabilizers\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "# Quit the browser\n",
    "driver.quit()\n",
    "\n",
    "# Convert the collected data into a pandas DataFrame\n",
    "df = pd.DataFrame(all_exercise_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/exercise_data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n"
   ],
   "id": "6613e90270686ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5028738d157a1b56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
