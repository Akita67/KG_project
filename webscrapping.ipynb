{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# All the muscles that can be trained + Other movements/competition\n",
   "id": "825d19716cb05e73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T10:35:14.567198Z",
     "start_time": "2025-02-21T10:35:10.376130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://exrx.net/Lists/Directory\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all divs with class \"col-sm-6\"\n",
    "exercise_divs = soup.find_all(\"div\", class_=\"col-sm-6\")\n",
    "\n",
    "# Extracting the a href names and their subsections\n",
    "exercise_data = []\n",
    "\n",
    "for div in exercise_divs:\n",
    "    links = div.find_all(\"a\")  # Find all <a> tags inside the div\n",
    "    for link in links:\n",
    "        name = link.text.strip()  # Extract the name\n",
    "        href = link[\"href\"] if \"href\" in link.attrs else None  # Extract the link\n",
    "        full_url = url + href if href else None  # Convert relative links to absolute\n",
    "        exercise_data.append((name, full_url))\n",
    "\n",
    "# Initialize variables\n",
    "data = []\n",
    "\n",
    "# Print the extracted data\n",
    "main = \"\"\n",
    "for name, link in exercise_data:\n",
    "    path_parts = link.split(\"#\")[-1:]  # Get the last part\n",
    "    if(path_parts[0] == link):\n",
    "        #print(f\"Exercise: {name}\")\n",
    "        main = name\n",
    "    else:\n",
    "        #print(f\"Exercise: {main} - Connected: {path_parts[0]}\")\n",
    "        data.append((main, path_parts[0]))  # Append main muscle and specific area\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Main Muscle\", \"Specific Area\"])\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "#file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/muscle_data.csv\"\n",
    "#df.to_csv(file_path, index=False)\n",
    "\n",
    "#print(f\"File saved: {file_path}\")\n",
    "# Close the browser\n",
    "driver.quit()"
   ],
   "id": "3f07625590c86881",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise: Neck\n",
      "Exercise: Neck - Connected: Sternocleidomastoid\n",
      "Exercise: Neck - Connected: Splenius\n",
      "Exercise: Shoulders\n",
      "Exercise: Shoulders - Connected: Anterior\n",
      "Exercise: Shoulders - Connected: Lateral\n",
      "Exercise: Shoulders - Connected: Posterior\n",
      "Exercise: Shoulders - Connected: Supraspinatus\n",
      "Exercise: Upper Arms\n",
      "Exercise: Upper Arms - Connected: Triceps\n",
      "Exercise: Upper Arms - Connected: Biceps\n",
      "Exercise: Upper Arms - Connected: Brachialis\n",
      "Exercise: Forearms\n",
      "Exercise: Forearms - Connected: Brachioradialis\n",
      "Exercise: Forearms - Connected: WristFlexors\n",
      "Exercise: Forearms - Connected: Extensors\n",
      "Exercise: Forearms - Connected: Pronators\n",
      "Exercise: Forearms - Connected: Supinators\n",
      "Exercise: Back\n",
      "Exercise: Back - Connected: General\n",
      "Exercise: Back - Connected: Latissimus\n",
      "Exercise: Back - Connected: UpperTrap\n",
      "Exercise: Back - Connected: MiddleTrap\n",
      "Exercise: Back - Connected: Trapezius\n",
      "Exercise: Back - Connected: UpperTrap\n",
      "Exercise: Back - Connected: Rhomboids\n",
      "Exercise: Back - Connected: Infraspinatus\n",
      "Exercise: Back - Connected: Subscapularis\n",
      "Exercise: Chest\n",
      "Exercise: Chest - Connected: General\n",
      "Exercise: Chest - Connected: General\n",
      "Exercise: Chest - Connected: Clavicular\n",
      "Exercise: Chest - Connected: Pectoralis\n",
      "Exercise: Chest - Connected: Anterior\n",
      "Exercise: Waist\n",
      "Exercise: Waist - Connected: Rectus\n",
      "Exercise: Waist - Connected: Transverse\n",
      "Exercise: Waist - Connected: Obliques\n",
      "Exercise: Waist - Connected: Quadratus\n",
      "Exercise: Waist - Connected: Erector\n",
      "Exercise: Hips\n",
      "Exercise: Hips - Connected: Gluteus\n",
      "Exercise: Hips - Connected: Abductors\n",
      "Exercise: Hips - Connected: Hip\n",
      "Exercise: Hips - Connected: DeepHip\n",
      "Exercise: Thighs\n",
      "Exercise: Thighs - Connected: Quadriceps\n",
      "Exercise: Thighs - Connected: Hamstrings\n",
      "Exercise: Thighs - Connected: HipAdductors\n",
      "Exercise: Calves\n",
      "Exercise: Calves - Connected: Gastrocnemius\n",
      "Exercise: Calves - Connected: Gastrocnemius\n",
      "Exercise: Calves - Connected: Soleus\n",
      "Exercise: Calves - Connected: Tibialis\n",
      "Exercise: Calves - Connected: Popliteus\n",
      "Exercise: Olympic-style Weightlifts\n",
      "Exercise: Plyometrics\n",
      "Exercise: Cardio & Conditioning\n",
      "Exercise: Kettlebell\n",
      "Exercise: Miscellaneous\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get the information about the muscle",
   "id": "6ac6042cca13c142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "892c9aa222d79a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Based on the specific muscle get all the exercises",
   "id": "dbcbc8296c376365"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "List = ['https://exrx.net/Lists/ExList/NeckWt','https://exrx.net/Lists/ExList/ShouldWt','https://exrx.net/Lists/ExList/ArmWt','https://exrx.net/Lists/ExList/ForeArmWt','https://exrx.net/Lists/ExList/BackWt','https://exrx.net/Lists/ExList/ChestWt','https://exrx.net/Lists/ExList/WaistWt','https://exrx.net/Lists/ExList/HipsWt','https://exrx.net/Lists/ExList/ThighWt','https://exrx.net/Lists/ExList/CalfWt']",
   "id": "ae1f935fb0bac0ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:31:19.279852Z",
     "start_time": "2025-02-21T16:31:13.329386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://exrx.net/Lists/ExList/NeckWt\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all divs with class \"col-sm-6\"\n",
    "exercise_divs = soup.find_all(\"div\", class_=[\"col-sm-6\",\"col-sm-12\"])\n",
    "\n",
    "# Extracting the a href names and their subsections\n",
    "exercise_data = []\n",
    "\n",
    "for div in exercise_divs:\n",
    "    links = div.find_all(\"a\")  # Find all <a> tags inside the div\n",
    "    for link in links:\n",
    "        name = link.text.strip()  # Extract the name\n",
    "        href = link[\"href\"] if \"href\" in link.attrs else None  # Extract the link\n",
    "        if href:\n",
    "            full_url = urllib.parse.urljoin(url, href)  # Convert relative links to absolute\n",
    "            cleaned_link = full_url.replace(\"/Lists/ExList/NeckWt../..\", \"\")  # Remove the unnecessary part\n",
    "        else:\n",
    "            full_url = None\n",
    "            cleaned_link = None\n",
    "\n",
    "        exercise_data.append((name, cleaned_link))\n",
    "\n",
    "# Print the extracted data\n",
    "list_exercises = []\n",
    "for name, link in exercise_data:\n",
    "    if(link!=None):\n",
    "        # Extract only the last two parts of the path\n",
    "        path_parts = link.split(\"/\")[-2:]  # Get the last two parts\n",
    "        short_link = \"/\".join(path_parts)  # Join them back into a string\n",
    "        print(f\"Exercise: {name} - Link: {link}\")\n",
    "        list_exercises.append(link)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ],
   "id": "390676636fd1cc2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise: Sternocleidomastoid - Link: https://exrx.net/Muscles/Sternocleidomastoid\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/CBNeckFlx\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckFlexionH\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVLateralNeckFlexionH\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckFlx\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckLtrFlx\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/WtNeckFlx\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/WtNeckLateralFlex\n",
      "Exercise: Wall Front Neck Bridge - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/BWWallFrontNeckBridge\n",
      "Exercise: Wall Side Neck Bridge - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/BWWallSideNeckBridge\n",
      "Exercise: Retraction - Link: https://exrx.net/Stretches/Sternocleidomastoid/NeckRetraction\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/STNeckFlexion\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/STNeckLateralFlexion\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/CBNeckFlx\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckFlexionH\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVLateralNeckFlexionH\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckFlx\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckLtrFlx\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/WtNeckFlx\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/WtNeckLateralFlex\n",
      "Exercise: Wall Front Neck Bridge - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/BWWallFrontNeckBridge\n",
      "Exercise: Wall Side Neck Bridge - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/BWWallSideNeckBridge\n",
      "Exercise: Retraction - Link: https://exrx.net/Stretches/Sternocleidomastoid/NeckRetraction\n",
      "Exercise: Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/STNeckFlexion\n",
      "Exercise: Lateral Neck Flexion - Link: https://exrx.net/WeightExercises/Sternocleidomastoid/STNeckLateralFlexion\n",
      "Exercise: Splenius - Link: https://exrx.net/Muscles/Splenius\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/CBNeckExt\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/LVNeckExtentionH\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/LVNeckExt\n",
      "Exercise: Seated Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/WtNeckExt\n",
      "Exercise: Harness - Link: https://exrx.net/WeightExercises/Splenius/WtNeckHarnessExt\n",
      "Exercise: Sternocleidomastoid - Link: https://exrx.net/Lists/ExList/NeckWt#Sternocleidomastoid\n",
      "Exercise: Neck Retraction - Link: https://exrx.net/WeightExercises/Splenius/BRNeckRetraction\n",
      "Exercise: Wall Rear Neck Bridge - Link: https://exrx.net/WeightExercises/Splenius/BWWallRearNeckBridge\n",
      "Exercise: Lying Neck Retraction - Link: https://exrx.net/WeightExercises/Splenius/LyingIsometricNeckRetr\n",
      "Exercise: Neck Extensor - Link: https://exrx.net/Stretches/Splenius/Neck\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/STNeckExtension\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/CBNeckExt\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/LVNeckExtentionH\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/LVNeckExt\n",
      "Exercise: Seated Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/WtNeckExt\n",
      "Exercise: Harness - Link: https://exrx.net/WeightExercises/Splenius/WtNeckHarnessExt\n",
      "Exercise: Sternocleidomastoid - Link: https://exrx.net/Lists/ExList/NeckWt#Sternocleidomastoid\n",
      "Exercise: Neck Retraction - Link: https://exrx.net/WeightExercises/Splenius/BRNeckRetraction\n",
      "Exercise: Wall Rear Neck Bridge - Link: https://exrx.net/WeightExercises/Splenius/BWWallRearNeckBridge\n",
      "Exercise: Lying Neck Retraction - Link: https://exrx.net/WeightExercises/Splenius/LyingIsometricNeckRetr\n",
      "Exercise: Neck Extensor - Link: https://exrx.net/Stretches/Splenius/Neck\n",
      "Exercise: Neck Extension - Link: https://exrx.net/WeightExercises/Splenius/STNeckExtension\n",
      "Exercise: Back - Link: https://exrx.net/Lists/ExList/BackWt\n",
      "Exercise: Upper    Trapezius - Link: https://exrx.net/Lists/ExList/BackWt#UpperTrap\n",
      "Exercise: Waist - Link: https://exrx.net/Lists/ExList/WaistWt\n",
      "Exercise: Erector    Spinae - Link: https://exrx.net/Lists/ExList/WaistWt#Erector\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get all the information out of an exercise",
   "id": "33eaeb863b8a04fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T10:40:24.205084Z",
     "start_time": "2025-02-21T10:40:19.229291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "#\n",
    "# # Set up options for headless Chrome\n",
    "# options = Options()\n",
    "# options.headless = True  # Enable headless mode for invisible operation\n",
    "# options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "#\n",
    "# # Set the path to the Chromedriver\n",
    "# DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "#\n",
    "# # Create a Service instance with the path to ChromeDriver\n",
    "# service = Service(DRIVER_PATH)\n",
    "#\n",
    "# # Initialize Chrome with the specified options\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "#\n",
    "# # Navigate to the website\n",
    "# #url = \"https://exrx.net/WeightExercises/PectoralSternal/BBBenchPress\"\n",
    "# url = \"https://exrx.net/WeightExercises/Sternocleidomastoid/LVNeckFlexionH\"\n",
    "#\n",
    "# driver.get(url)\n",
    "#\n",
    "# # Get page source and parse it with BeautifulSoup\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#\n",
    "# # Close the browser\n",
    "# driver.quit()\n",
    "#\n",
    "# # Extract data\n",
    "# utility = soup.find('strong', string='Utility:').find_next('td').string.strip()\n",
    "# mechanics = soup.find('strong', string='Mechanics:').find_next('td').string.strip()\n",
    "# force = soup.find('strong', string='Force:').find_next('td').string.strip()\n",
    "# preparation = soup.find('strong', string='Preparation').find_next('p').string.strip()\n",
    "# execution = soup.find('strong', string='Execution').find_next('p').string.strip()\n",
    "#\n",
    "# # Extract muscles information\n",
    "# target = [li.string.strip() for li in soup.find('strong', string='Target').find_next('ul').find_all('li')]\n",
    "# synergists = [li.string.strip() for li in soup.find('strong', string='Synergists').find_next('ul').find_all('li')]\n",
    "# dynamic_stabilizers = [li.string.strip() for li in soup.find('strong', string='Dynamic Stabilizers').find_next('ul').find_all('li')]\n",
    "#\n",
    "# # Create pandas DataFrame\n",
    "# data = {\n",
    "#     'Utility': [utility],\n",
    "#     'Mechanics': [mechanics],\n",
    "#     'Force': [force],\n",
    "#     'Preparation': [preparation],\n",
    "#     'Execution': [execution],\n",
    "#     'Target': [target],\n",
    "#     'Synergists': [synergists],\n",
    "#     'Dynamic Stabilizers': [dynamic_stabilizers]\n",
    "# }\n",
    "#\n",
    "# df = pd.DataFrame(data)\n",
    "#\n",
    "# # Print the DataFrame\n",
    "# print(df)\n",
    "#\n",
    "# # Save to CSV\n",
    "# file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/exercise_data.csv\"\n",
    "# df.to_csv(file_path, index=False)\n",
    "#\n",
    "# print(f\"File saved: {file_path}\")"
   ],
   "id": "3dbcddf9bca05207",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 34\u001B[0m\n\u001B[0;32m     31\u001B[0m driver\u001B[38;5;241m.\u001B[39mquit()\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Extract data\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m utility \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrong\u001B[39m\u001B[38;5;124m'\u001B[39m, string\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUtility:\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfind_next(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtd\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstring\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m     35\u001B[0m mechanics \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrong\u001B[39m\u001B[38;5;124m'\u001B[39m, string\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMechanics:\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfind_next(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtd\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstring\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m     36\u001B[0m force \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrong\u001B[39m\u001B[38;5;124m'\u001B[39m, string\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mForce:\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfind_next(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtd\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstring\u001B[38;5;241m.\u001B[39mstrip()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:33:37.865107Z",
     "start_time": "2025-02-21T16:33:32.654599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the target website\n",
    "#url = \"https://exrx.net/WeightExercises/GluteusMaximus/SLSquat\"\n",
    "url = \"https://exrx.net/WeightExercises/Sternocleidomastoid/LVLateralNeckFlexionH\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Ensure the driver quits after loading the page\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# Locate the string 'Utility', find the next <a> tag, and extract its text\n",
    "def extract_utility_name(soup,value):\n",
    "    \"\"\"\n",
    "    Finds the string 'Utility' in a <strong> tag, gets the next <a> tag,\n",
    "    and extracts its text.\n",
    "    \"\"\"\n",
    "    element = soup.find('strong', string=value)\n",
    "    if element:\n",
    "        next_a = element.find_next('a')  # Find the next <a> tag\n",
    "        if next_a:\n",
    "            return next_a.text.strip()  # Extract the text inside the <a> tag\n",
    "    return None  # Return None if not found\n",
    "\n",
    "\n",
    "# Call the function and fetch the utility name\n",
    "utility = extract_utility_name(soup, 'Utility:')\n",
    "mechanics = extract_utility_name(soup, 'Mechanics:')\n",
    "force = extract_utility_name(soup, 'Force:')\n",
    "preparation = soup.find('strong', string='Preparation').find_next('p').string.strip()\n",
    "execution = soup.find('strong', string='Execution').find_next('p').string.strip()\n",
    "\n",
    "# Extract muscles information\n",
    "target = [li.string.strip() for li in soup.find('strong', string='Target').find_next('ul').find_all('li')]\n",
    "print(target)\n",
    "synergists = [li.string.strip() for li in soup.find('strong', string='Synergists').find_next('ul').find_all('li')]\n",
    "print(synergists)\n",
    "dynamic_stabilizers = [li.string.strip() for li in soup.find('strong', string=lambda x: x in ['Dynamic Stabilizers', 'Stabilizers']).find_next('ul').find_all('li')]\n",
    "\n",
    "print(dynamic_stabilizers)\n",
    "\n",
    "\n",
    "# Create pandas DataFrame\n",
    "data = {\n",
    "    'Utility': [utility],\n",
    "    'Mechanics': [mechanics],\n",
    "    'Force': [force],\n",
    "    'Preparation': [preparation],\n",
    "    'Execution': [execution],\n",
    "    'Target': [target],\n",
    "    'Synergists': [synergists],\n",
    "    'Dynamic Stabilizers': [dynamic_stabilizers]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/exercise_solo_data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n"
   ],
   "id": "357d0c8c33ed6f2c",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 51\u001B[0m\n\u001B[0;32m     49\u001B[0m mechanics \u001B[38;5;241m=\u001B[39m extract_utility_name(soup, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMechanics:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     50\u001B[0m force \u001B[38;5;241m=\u001B[39m extract_utility_name(soup, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mForce:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 51\u001B[0m preparation \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrong\u001B[39m\u001B[38;5;124m'\u001B[39m, string\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPreparation\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfind_next(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mp\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstring\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m     52\u001B[0m execution \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrong\u001B[39m\u001B[38;5;124m'\u001B[39m, string\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExecution\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfind_next(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mp\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstring\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Extract muscles information\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:32:43.325398Z",
     "start_time": "2025-02-21T16:31:32.274799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# List of URLs to scrape\n",
    "url_list = [\n",
    "    \"https://exrx.net/WeightExercises/Sternocleidomastoid/CBNeckFlx\",\n",
    "    \"https://exrx.net/WeightExercises/PectoralSternal/BBBenchPress\",\n",
    "    \"https://exrx.net/WeightExercises/RectusAbdominis/BWCrunch\",\n",
    "    \"https://exrx.net/WeightExercises/GluteusMaximus/SLSquat\"\n",
    "]\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "# Function to extract utility data\n",
    "def extract_utility_name(soup, value):\n",
    "    \"\"\"\n",
    "    Finds the string in a <strong> tag, gets the next <a> tag,\n",
    "    and extracts its text.\n",
    "    \"\"\"\n",
    "    element = soup.find('strong', string=value)\n",
    "    if element:\n",
    "        next_a = element.find_next('a')  # Find the next <a> tag\n",
    "        if next_a:\n",
    "            return next_a.text.strip()  # Extract the text inside the <a> tag\n",
    "    return None  # Return None if not found\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all rows of data\n",
    "all_exercise_data = []\n",
    "\n",
    "# Iterate through URLs in the list\n",
    "for url in list_exercises:\n",
    "    try:\n",
    "        # Navigate to the target website\n",
    "        driver.get(url)\n",
    "\n",
    "        # Get page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract exercise details\n",
    "        utility = extract_utility_name(soup, 'Utility:')\n",
    "        mechanics = extract_utility_name(soup, 'Mechanics:')\n",
    "        force = extract_utility_name(soup, 'Force:')\n",
    "\n",
    "        # Extract preparation and execution details\n",
    "        preparation = soup.find('strong', string='Preparation').find_next('p').string.strip()\n",
    "        execution = soup.find('strong', string='Execution').find_next('p').string.strip()\n",
    "\n",
    "        # Extract muscles information\n",
    "        target = [li.string.strip() for li in soup.find('strong', string='Target').find_next('ul').find_all('li')]\n",
    "        synergists = [li.string.strip() for li in\n",
    "                      soup.find('strong', string='Synergists').find_next('ul').find_all('li')]\n",
    "        dynamic_stabilizers = [\n",
    "            li.string.strip()\n",
    "            for li in soup.find('strong', string=lambda x: x in ['Dynamic Stabilizers', 'Stabilizers'])\n",
    "            .find_next('ul').find_all('li')\n",
    "        ]\n",
    "\n",
    "        # Add the extracted data to the list\n",
    "        all_exercise_data.append({\n",
    "            'URL': url,\n",
    "            'Utility': utility,\n",
    "            'Mechanics': mechanics,\n",
    "            'Force': force,\n",
    "            'Preparation': preparation,\n",
    "            'Execution': execution,\n",
    "            'Target': target,\n",
    "            'Synergists': synergists,\n",
    "            'Dynamic Stabilizers': dynamic_stabilizers\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "# Quit the browser\n",
    "driver.quit()\n",
    "\n",
    "# Convert the collected data into a pandas DataFrame\n",
    "df = pd.DataFrame(all_exercise_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/exercise_data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n"
   ],
   "id": "5bf007815d2ade49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://exrx.net/Muscles/Sternocleidomastoid: 'NoneType' object has no attribute 'find_next'\n",
      "Failed to scrape https://exrx.net/WeightExercises/Sternocleidomastoid/LVLateralNeckFlexionH: 'NoneType' object has no attribute 'strip'\n",
      "Failed to scrape https://exrx.net/WeightExercises/Sternocleidomastoid/WtNeckLateralFlex: 'NoneType' object has no attribute 'find_next'\n",
      "Failed to scrape https://exrx.net/Stretches/Sternocleidomastoid/NeckRetraction: 'NoneType' object has no attribute 'find_next'\n",
      "Failed to scrape https://exrx.net/WeightExercises/Sternocleidomastoid/STNeckLateralFlexion: 'NoneType' object has no attribute 'find_next'\n",
      "Failed to scrape https://exrx.net/WeightExercises/Sternocleidomastoid/LVLateralNeckFlexionH: 'NoneType' object has no attribute 'strip'\n",
      "Failed to scrape https://exrx.net/WeightExercises/Sternocleidomastoid/WtNeckLateralFlex: 'NoneType' object has no attribute 'find_next'\n",
      "Failed to scrape https://exrx.net/Stretches/Sternocleidomastoid/NeckRetraction: 'NoneType' object has no attribute 'find_next'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 51\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m list_exercises:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     50\u001B[0m         \u001B[38;5;66;03m# Navigate to the target website\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m         driver\u001B[38;5;241m.\u001B[39mget(url)\n\u001B[0;32m     53\u001B[0m         \u001B[38;5;66;03m# Get page source and parse it with BeautifulSoup\u001B[39;00m\n\u001B[0;32m     54\u001B[0m         soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(driver\u001B[38;5;241m.\u001B[39mpage_source, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:454\u001B[0m, in \u001B[0;36mWebDriver.get\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    436\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    437\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001B[39;00m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;124;03m    tab.\u001B[39;00m\n\u001B[0;32m    439\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 454\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute(Command\u001B[38;5;241m.\u001B[39mGET, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m: url})\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:427\u001B[0m, in \u001B[0;36mWebDriver.execute\u001B[1;34m(self, driver_command, params)\u001B[0m\n\u001B[0;32m    424\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msessionId\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[0;32m    425\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msessionId\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msession_id\n\u001B[1;32m--> 427\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_executor\u001B[38;5;241m.\u001B[39mexecute(driver_command, params)\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response:\n\u001B[0;32m    429\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_handler\u001B[38;5;241m.\u001B[39mcheck_response(response)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001B[0m, in \u001B[0;36mRemoteConnection.execute\u001B[1;34m(self, command, params)\u001B[0m\n\u001B[0;32m    402\u001B[0m trimmed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trim_large_entries(params)\n\u001B[0;32m    403\u001B[0m LOGGER\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, command_info[\u001B[38;5;241m0\u001B[39m], url, \u001B[38;5;28mstr\u001B[39m(trimmed))\n\u001B[1;32m--> 404\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(command_info[\u001B[38;5;241m0\u001B[39m], url, body\u001B[38;5;241m=\u001B[39mdata)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001B[0m, in \u001B[0;36mRemoteConnection._request\u001B[1;34m(self, method, url, body)\u001B[0m\n\u001B[0;32m    425\u001B[0m     body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_config\u001B[38;5;241m.\u001B[39mkeep_alive:\n\u001B[1;32m--> 428\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conn\u001B[38;5;241m.\u001B[39mrequest(method, url, body\u001B[38;5;241m=\u001B[39mbody, headers\u001B[38;5;241m=\u001B[39mheaders, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_config\u001B[38;5;241m.\u001B[39mtimeout)\n\u001B[0;32m    429\u001B[0m     statuscode \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mstatus\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:78\u001B[0m, in \u001B[0;36mRequestMethods.request\u001B[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_encode_url(\n\u001B[0;32m     75\u001B[0m         method, url, fields\u001B[38;5;241m=\u001B[39mfields, headers\u001B[38;5;241m=\u001B[39mheaders, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39murlopen_kw\n\u001B[0;32m     76\u001B[0m     )\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_encode_body(\n\u001B[0;32m     79\u001B[0m         method, url, fields\u001B[38;5;241m=\u001B[39mfields, headers\u001B[38;5;241m=\u001B[39mheaders, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39murlopen_kw\n\u001B[0;32m     80\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:170\u001B[0m, in \u001B[0;36mRequestMethods.request_encode_body\u001B[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001B[0m\n\u001B[0;32m    167\u001B[0m extra_kw[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mupdate(headers)\n\u001B[0;32m    168\u001B[0m extra_kw\u001B[38;5;241m.\u001B[39mupdate(urlopen_kw)\n\u001B[1;32m--> 170\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murlopen(method, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mextra_kw)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:376\u001B[0m, in \u001B[0;36mPoolManager.urlopen\u001B[1;34m(self, method, url, redirect, **kw)\u001B[0m\n\u001B[0;32m    374\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(method, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 376\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(method, u\u001B[38;5;241m.\u001B[39mrequest_uri, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    378\u001B[0m redirect_location \u001B[38;5;241m=\u001B[39m redirect \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mget_redirect_location()\n\u001B[0;32m    379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m redirect_location:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_proxy(conn)\n\u001B[0;32m    713\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[1;32m--> 714\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    715\u001B[0m     conn,\n\u001B[0;32m    716\u001B[0m     method,\n\u001B[0;32m    717\u001B[0m     url,\n\u001B[0;32m    718\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    719\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    720\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    721\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    722\u001B[0m )\n\u001B[0;32m    724\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[0;32m    726\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n\u001B[0;32m    728\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    461\u001B[0m             httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m             \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m             \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m             \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[1;32m--> 466\u001B[0m             six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;66;03m# Python 3\u001B[39;00m\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 461\u001B[0m         httplib_response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[0;32m    462\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    463\u001B[0m         \u001B[38;5;66;03m# Remove the TypeError from the exception chain in\u001B[39;00m\n\u001B[0;32m    464\u001B[0m         \u001B[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001B[39;00m\n\u001B[0;32m    465\u001B[0m         \u001B[38;5;66;03m# Otherwise it looks like a bug in the code.\u001B[39;00m\n\u001B[0;32m    466\u001B[0m         six\u001B[38;5;241m.\u001B[39mraise_from(e, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1375\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1374\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1375\u001B[0m         response\u001B[38;5;241m.\u001B[39mbegin()\n\u001B[0;32m   1376\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1377\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_status()\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:57:08.356091Z",
     "start_time": "2025-02-21T16:57:04.570063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #!pip install selenium\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "#\n",
    "# # Set up options for headless Chrome\n",
    "# options = Options()\n",
    "# options.headless = True  # Enable headless mode for invisible operation\n",
    "# options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "#\n",
    "# # Set the path to the Chromedriver\n",
    "# DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "#\n",
    "# # Create a Service instance with the path to ChromeDriver\n",
    "# service = Service(DRIVER_PATH)\n",
    "#\n",
    "# # Initialize Chrome with the specified options\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "#\n",
    "# # Navigate to the website\n",
    "# url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "# driver.get(url)\n",
    "#\n",
    "# # Get page source and parse it with BeautifulSoup\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#\n",
    "# # Find the section with the detailed nutrition facts\n",
    "# section = soup.find('section', id='detailed-nutrition-facts')\n",
    "# if not section:\n",
    "#     raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "#\n",
    "# # Find the table with nutrition data\n",
    "# table = section.find('table', id='detailedNutritionDataTable')\n",
    "# if not table:\n",
    "#     raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "#\n",
    "# # Extract all table rows (tr elements)\n",
    "# rows = table.find_all('tr')\n",
    "#\n",
    "# # Extract table data into a list\n",
    "# data = []\n",
    "# current_category = None\n",
    "# for row in rows:\n",
    "#     header = row.find('th', class_='headerRow', colspan='3')\n",
    "#     if header:\n",
    "#         current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "#     cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "#     if cols:\n",
    "#         data.append([current_category] + cols)  # Prepend category to row data\n",
    "#\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "#\n",
    "# # Save to CSV\n",
    "# file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\"\n",
    "# df.to_csv(file_path, index=False)\n",
    "#\n",
    "# print(f\"File saved: {file_path}\")\n",
    "#\n",
    "# # Close the Selenium driver\n",
    "# driver.quit()"
   ],
   "id": "30c183556111afc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0                  1       2     3\n",
      "0    Macronutrients     Macronutrients    None  None\n",
      "1    Macronutrients  -- = missing data     %DV  None\n",
      "2    Macronutrients             Weight    100g      \n",
      "3    Macronutrients    Calories (kcal)     585   29%\n",
      "4    Macronutrients                Fat   60.4g   77%\n",
      "..              ...                ...     ...   ...\n",
      "181     Amino Acids      Aspartic acid   984mg      \n",
      "182     Amino Acids            Betaine     2mg      \n",
      "183     Amino Acids      Glutamic acid  1605mg      \n",
      "184     Amino Acids     Hydroxyproline    26mg      \n",
      "185     Amino Acids             Serine   435mg      \n",
      "\n",
      "[186 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:16:02.775286Z",
     "start_time": "2025-02-21T17:15:58.895678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find the section with the detailed nutrition facts\n",
    "section = soup.find('section', id='detailed-nutrition-facts')\n",
    "if not section:\n",
    "    raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "\n",
    "# Find the table with nutrition data\n",
    "table = section.find('table', id='detailedNutritionDataTable')\n",
    "if not table:\n",
    "    raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "\n",
    "# Extract all table rows (tr elements)\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract table data into a list\n",
    "data = []\n",
    "current_category = None\n",
    "for row in rows:\n",
    "    header = row.find('th', class_='headerRow', colspan='3')\n",
    "    if header:\n",
    "        current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "    cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "    if cols:\n",
    "        data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "# Remove rows where the first and second column values are the same\n",
    "drop_indices = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i, 0] == df.iloc[i, 1]:  # Compare \"Name\" and \"Measurement\"\n",
    "        drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "\n",
    "df.drop(index=drop_indices, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicate names in the \"Name\" column\n",
    "df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()\n"
   ],
   "id": "e3da45bacd2dd9c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Categories             Name Measurement Daily Value\n",
      "0    Macronutrients           Weight        100g            \n",
      "1    Macronutrients  Calories (kcal)         585         29%\n",
      "2    Macronutrients              Fat       60.4g         77%\n",
      "3    Macronutrients          Protein       10.1g         20%\n",
      "4    Macronutrients     Carbohydrate       0.32g          0%\n",
      "..              ...              ...         ...         ...\n",
      "160     Amino Acids    Aspartic acid       984mg            \n",
      "161     Amino Acids          Betaine         2mg            \n",
      "162     Amino Acids    Glutamic acid      1605mg            \n",
      "163     Amino Acids   Hydroxyproline        26mg            \n",
      "164     Amino Acids           Serine       435mg            \n",
      "\n",
      "[165 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:39:54.640893Z",
     "start_time": "2025-02-21T17:39:50.780598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website 4541 to 749420\n",
    "url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find the section with the detailed nutrition facts\n",
    "section = soup.find('section', id='detailed-nutrition-facts')\n",
    "if not section:\n",
    "    raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "\n",
    "# Find the table with nutrition data\n",
    "table = section.find('table', id='detailedNutritionDataTable')\n",
    "if not table:\n",
    "    raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "\n",
    "# Extract all table rows (tr elements)\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract table data into a list\n",
    "data = []\n",
    "current_category = None\n",
    "for row in rows:\n",
    "    header = row.find('th', class_='headerRow', colspan='3')\n",
    "    if header:\n",
    "        current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "    cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "    if cols:\n",
    "        data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "# Remove rows where the first and second column values are the same\n",
    "drop_indices = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i, 1] == df.iloc[i, 2]:  # Compare \"Name\" and \"Measurement\"\n",
    "        drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "    elif df.iloc[i,1] == \"-- = missing data\":\n",
    "        drop_indices.extend([i])\n",
    "\n",
    "df.drop(index=drop_indices, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicate names in the \"Name\" column\n",
    "df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Handle subcategories\n",
    "filtered_data = []\n",
    "current_category = None\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):  # Detect subcategory\n",
    "        current_category = row[\"Name\"]  # Update current subcategory\n",
    "    else:\n",
    "        filtered_data.append([current_category] + row[1:].tolist())  # Assign subcategory\n",
    "\n",
    "# Create updated DataFrame\n",
    "df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()\n"
   ],
   "id": "2d03965d6385cba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Categories             Name Measurement Daily Value\n",
      "0               Macronutrients           Weight        100g            \n",
      "1               Macronutrients  Calories (kcal)         585         29%\n",
      "2               Macronutrients              Fat       60.4g         77%\n",
      "3               Macronutrients          Protein       10.1g         20%\n",
      "4               Macronutrients     Carbohydrate       0.32g          0%\n",
      "..                         ...              ...         ...         ...\n",
      "151  Non-Essential Amino Acids    Aspartic acid       984mg            \n",
      "152  Non-Essential Amino Acids          Betaine         2mg            \n",
      "153  Non-Essential Amino Acids    Glutamic acid      1605mg            \n",
      "154  Non-Essential Amino Acids   Hydroxyproline        26mg            \n",
      "155  Non-Essential Amino Acids           Serine       435mg            \n",
      "\n",
      "[156 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T19:32:25.585579Z",
     "start_time": "2025-02-21T19:32:21.740785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website 4541 to 749420\n",
    "url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Extract food name from the <h1 class=\"nutritionFactsTitle\"> tag\n",
    "food_name_tag = soup.find('h1', class_='nutritionFactsTitle')\n",
    "if food_name_tag:\n",
    "    food_name = food_name_tag.get_text(strip=True)\n",
    "else:\n",
    "    raise ValueError(\"Could not find the food name.\")\n",
    "\n",
    "# Clean the food name to make it safe for a file path (remove special characters and spaces)\n",
    "food_name = re.sub(r',', '', food_name)\n",
    "safe_food_name = re.sub(r'[^a-zA-Z0-9_\\-]', '_', food_name)\n",
    "\n",
    "# Set the file path based on the food name\n",
    "file_path = f\"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/{safe_food_name}_nutrition.csv\"\n",
    "\n",
    "# Find the section with the detailed nutrition facts\n",
    "section = soup.find('section', id='detailed-nutrition-facts')\n",
    "if not section:\n",
    "    raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "\n",
    "# Find the table with nutrition data\n",
    "table = section.find('table', id='detailedNutritionDataTable')\n",
    "if not table:\n",
    "    raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "\n",
    "# Extract all table rows (tr elements)\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract table data into a list\n",
    "data = []\n",
    "current_category = None\n",
    "for row in rows:\n",
    "    header = row.find('th', class_='headerRow', colspan='3')\n",
    "    if header:\n",
    "        current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "    cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "    if cols:\n",
    "        data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "# Remove rows where the first and second column values are the same\n",
    "drop_indices = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i, 1] == df.iloc[i, 2]:  # Compare \"Name\" and \"Measurement\"\n",
    "        drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "    elif df.iloc[i, 1] == \"-- = missing data\":\n",
    "        drop_indices.extend([i])\n",
    "\n",
    "df.drop(index=drop_indices, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicate names in the \"Name\" column\n",
    "df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Handle subcategories\n",
    "filtered_data = []\n",
    "current_category = None\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):  # Detect subcategory\n",
    "        current_category = row[\"Name\"]  # Update current subcategory\n",
    "    else:\n",
    "        filtered_data.append([current_category] + row[1:].tolist())  # Assign subcategory\n",
    "\n",
    "# Create updated DataFrame\n",
    "df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()"
   ],
   "id": "113449adbd2b5222",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Categories             Name Measurement Daily Value\n",
      "0               Macronutrients           Weight        100g            \n",
      "1               Macronutrients  Calories (kcal)         585         29%\n",
      "2               Macronutrients              Fat       60.4g         77%\n",
      "3               Macronutrients          Protein       10.1g         20%\n",
      "4               Macronutrients     Carbohydrate       0.32g          0%\n",
      "..                         ...              ...         ...         ...\n",
      "151  Non-Essential Amino Acids    Aspartic acid       984mg            \n",
      "152  Non-Essential Amino Acids          Betaine         2mg            \n",
      "153  Non-Essential Amino Acids    Glutamic acid      1605mg            \n",
      "154  Non-Essential Amino Acids   Hydroxyproline        26mg            \n",
      "155  Non-Essential Amino Acids           Serine       435mg            \n",
      "\n",
      "[156 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/Pork_fresh_composite_of_separable_fat_with_added_solution_cooked_nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Loop over the range of page IDs (4539 to 749420) 21112\n",
    "for page_id in range(17930, 100000):\n",
    "    # Build the URL for each page\n",
    "    url = f\"https://tools.myfooddata.com/nutrition-facts/{page_id}/wt2/1\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Check if the page exists (i.e., doesn't show the \"foodNotFound\" message)\n",
    "    food_not_found_tag = soup.find('h1', id='foodNotFound')\n",
    "    if food_not_found_tag and food_not_found_tag.get_text(strip=True) == \"Sorry, we can't find that food. Please use the search box to search for another one.\":\n",
    "        print(f\"Food not found for ID: {page_id}, skipping...\")\n",
    "        time.sleep(1)\n",
    "        continue  # Skip to the next iteration if food is not found\n",
    "\n",
    "    # Extract food name from the <h1 class=\"nutritionFactsTitle\"> tag\n",
    "    food_name_tag = soup.find('h1', class_='nutritionFactsTitle')\n",
    "    if food_name_tag:\n",
    "        food_name = food_name_tag.get_text(strip=True)\n",
    "    else:\n",
    "        print(f\"Could not find food name for ID: {page_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Clean the food name to make it safe for a file path (remove special characters and spaces)\n",
    "    food_name = re.sub(r',', '', food_name)\n",
    "    safe_food_name = re.sub(r'[^a-zA-Z0-9_\\-]', '_', food_name)\n",
    "\n",
    "    # Set the file path based on the food name\n",
    "    file_path = f\"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/Nutrition/{safe_food_name + '_' + str(page_id)}.csv\"\n",
    "\n",
    "    # Find the section with the detailed nutrition facts\n",
    "    section = soup.find('section', id='detailed-nutrition-facts')\n",
    "    if not section:\n",
    "        print(f\"Could not find the nutrition facts section for ID: {page_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Find the table with nutrition data\n",
    "    table = section.find('table', id='detailedNutritionDataTable')\n",
    "    if not table:\n",
    "        print(f\"Could not find the nutrition table for ID: {page_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract all table rows (tr elements)\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Extract table data into a list\n",
    "    data = []\n",
    "    current_category = None\n",
    "    for row in rows:\n",
    "        header = row.find('th', class_='headerRow', colspan='3')\n",
    "        if header:\n",
    "            current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "        cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "        if cols:\n",
    "            data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "    # Remove rows where the first and second column values are the same\n",
    "    drop_indices = []\n",
    "    for i in range(len(df) - 1):\n",
    "        if df.iloc[i, 1] == df.iloc[i, 2]:  # Compare \"Name\" and \"Measurement\"\n",
    "            drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "        elif df.iloc[i, 1] == \"-- = missing data\":\n",
    "            drop_indices.extend([i])\n",
    "\n",
    "    df.drop(index=drop_indices, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Remove duplicate names in the \"Name\" column\n",
    "    df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Handle subcategories\n",
    "    filtered_data = []\n",
    "    current_category = None\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):  # Detect subcategory\n",
    "            current_category = row[\"Name\"]  # Update current subcategory\n",
    "        else:\n",
    "            filtered_data.append([current_category] + row[1:].tolist())  # Assign subcategory\n",
    "\n",
    "    # Create updated DataFrame\n",
    "    df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "    print(f\"Saving data for food: {food_name} (ID: {page_id})\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    #print(f\"File saved: {file_path}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()"
   ],
   "id": "50316b682098b2fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9297c9d644fa20e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
