{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multiple testing version",
   "id": "b31f8ab1064f01c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:57:08.356091Z",
     "start_time": "2025-02-21T16:57:04.570063Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0                  1       2     3\n",
      "0    Macronutrients     Macronutrients    None  None\n",
      "1    Macronutrients  -- = missing data     %DV  None\n",
      "2    Macronutrients             Weight    100g      \n",
      "3    Macronutrients    Calories (kcal)     585   29%\n",
      "4    Macronutrients                Fat   60.4g   77%\n",
      "..              ...                ...     ...   ...\n",
      "181     Amino Acids      Aspartic acid   984mg      \n",
      "182     Amino Acids            Betaine     2mg      \n",
      "183     Amino Acids      Glutamic acid  1605mg      \n",
      "184     Amino Acids     Hydroxyproline    26mg      \n",
      "185     Amino Acids             Serine   435mg      \n",
      "\n",
      "[186 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 17,
   "source": [
    "# #!pip install selenium\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "#\n",
    "# # Set up options for headless Chrome\n",
    "# options = Options()\n",
    "# options.headless = True  # Enable headless mode for invisible operation\n",
    "# options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "#\n",
    "# # Set the path to the Chromedriver\n",
    "# DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "#\n",
    "# # Create a Service instance with the path to ChromeDriver\n",
    "# service = Service(DRIVER_PATH)\n",
    "#\n",
    "# # Initialize Chrome with the specified options\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "#\n",
    "# # Navigate to the website\n",
    "# url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "# driver.get(url)\n",
    "#\n",
    "# # Get page source and parse it with BeautifulSoup\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#\n",
    "# # Find the section with the detailed nutrition facts\n",
    "# section = soup.find('section', id='detailed-nutrition-facts')\n",
    "# if not section:\n",
    "#     raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "#\n",
    "# # Find the table with nutrition data\n",
    "# table = section.find('table', id='detailedNutritionDataTable')\n",
    "# if not table:\n",
    "#     raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "#\n",
    "# # Extract all table rows (tr elements)\n",
    "# rows = table.find_all('tr')\n",
    "#\n",
    "# # Extract table data into a list\n",
    "# data = []\n",
    "# current_category = None\n",
    "# for row in rows:\n",
    "#     header = row.find('th', class_='headerRow', colspan='3')\n",
    "#     if header:\n",
    "#         current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "#     cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "#     if cols:\n",
    "#         data.append([current_category] + cols)  # Prepend category to row data\n",
    "#\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "#\n",
    "# # Save to CSV\n",
    "# file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\"\n",
    "# df.to_csv(file_path, index=False)\n",
    "#\n",
    "# print(f\"File saved: {file_path}\")\n",
    "#\n",
    "# # Close the Selenium driver\n",
    "# driver.quit()"
   ],
   "id": "30c183556111afc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:16:02.775286Z",
     "start_time": "2025-02-21T17:15:58.895678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find the section with the detailed nutrition facts\n",
    "section = soup.find('section', id='detailed-nutrition-facts')\n",
    "if not section:\n",
    "    raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "\n",
    "# Find the table with nutrition data\n",
    "table = section.find('table', id='detailedNutritionDataTable')\n",
    "if not table:\n",
    "    raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "\n",
    "# Extract all table rows (tr elements)\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract table data into a list\n",
    "data = []\n",
    "current_category = None\n",
    "for row in rows:\n",
    "    header = row.find('th', class_='headerRow', colspan='3')\n",
    "    if header:\n",
    "        current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "    cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "    if cols:\n",
    "        data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "# Remove rows where the first and second column values are the same\n",
    "drop_indices = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i, 0] == df.iloc[i, 1]:  # Compare \"Name\" and \"Measurement\"\n",
    "        drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "\n",
    "df.drop(index=drop_indices, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicate names in the \"Name\" column\n",
    "df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()\n"
   ],
   "id": "e3da45bacd2dd9c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Categories             Name Measurement Daily Value\n",
      "0    Macronutrients           Weight        100g            \n",
      "1    Macronutrients  Calories (kcal)         585         29%\n",
      "2    Macronutrients              Fat       60.4g         77%\n",
      "3    Macronutrients          Protein       10.1g         20%\n",
      "4    Macronutrients     Carbohydrate       0.32g          0%\n",
      "..              ...              ...         ...         ...\n",
      "160     Amino Acids    Aspartic acid       984mg            \n",
      "161     Amino Acids          Betaine         2mg            \n",
      "162     Amino Acids    Glutamic acid      1605mg            \n",
      "163     Amino Acids   Hydroxyproline        26mg            \n",
      "164     Amino Acids           Serine       435mg            \n",
      "\n",
      "[165 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T17:39:54.640893Z",
     "start_time": "2025-02-21T17:39:50.780598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website 4541 to 749420\n",
    "url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find the section with the detailed nutrition facts\n",
    "section = soup.find('section', id='detailed-nutrition-facts')\n",
    "if not section:\n",
    "    raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "\n",
    "# Find the table with nutrition data\n",
    "table = section.find('table', id='detailedNutritionDataTable')\n",
    "if not table:\n",
    "    raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "\n",
    "# Extract all table rows (tr elements)\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract table data into a list\n",
    "data = []\n",
    "current_category = None\n",
    "for row in rows:\n",
    "    header = row.find('th', class_='headerRow', colspan='3')\n",
    "    if header:\n",
    "        current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "    cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "    if cols:\n",
    "        data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "# Remove rows where the first and second column values are the same\n",
    "drop_indices = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i, 1] == df.iloc[i, 2]:  # Compare \"Name\" and \"Measurement\"\n",
    "        drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "    elif df.iloc[i,1] == \"-- = missing data\":\n",
    "        drop_indices.extend([i])\n",
    "\n",
    "df.drop(index=drop_indices, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicate names in the \"Name\" column\n",
    "df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Handle subcategories\n",
    "filtered_data = []\n",
    "current_category = None\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):  # Detect subcategory\n",
    "        current_category = row[\"Name\"]  # Update current subcategory\n",
    "    else:\n",
    "        filtered_data.append([current_category] + row[1:].tolist())  # Assign subcategory\n",
    "\n",
    "# Create updated DataFrame\n",
    "df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()\n"
   ],
   "id": "2d03965d6385cba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Categories             Name Measurement Daily Value\n",
      "0               Macronutrients           Weight        100g            \n",
      "1               Macronutrients  Calories (kcal)         585         29%\n",
      "2               Macronutrients              Fat       60.4g         77%\n",
      "3               Macronutrients          Protein       10.1g         20%\n",
      "4               Macronutrients     Carbohydrate       0.32g          0%\n",
      "..                         ...              ...         ...         ...\n",
      "151  Non-Essential Amino Acids    Aspartic acid       984mg            \n",
      "152  Non-Essential Amino Acids          Betaine         2mg            \n",
      "153  Non-Essential Amino Acids    Glutamic acid      1605mg            \n",
      "154  Non-Essential Amino Acids   Hydroxyproline        26mg            \n",
      "155  Non-Essential Amino Acids           Serine       435mg            \n",
      "\n",
      "[156 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T19:32:25.585579Z",
     "start_time": "2025-02-21T19:32:21.740785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Navigate to the website 4541 to 749420\n",
    "url = \"https://tools.myfooddata.com/nutrition-facts/10000/wt2/1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Get page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Extract food name from the <h1 class=\"nutritionFactsTitle\"> tag\n",
    "food_name_tag = soup.find('h1', class_='nutritionFactsTitle')\n",
    "if food_name_tag:\n",
    "    food_name = food_name_tag.get_text(strip=True)\n",
    "else:\n",
    "    raise ValueError(\"Could not find the food name.\")\n",
    "\n",
    "# Clean the food name to make it safe for a file path (remove special characters and spaces)\n",
    "food_name = re.sub(r',', '', food_name)\n",
    "safe_food_name = re.sub(r'[^a-zA-Z0-9_\\-]', '_', food_name)\n",
    "\n",
    "# Set the file path based on the food name\n",
    "file_path = f\"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/{safe_food_name}_nutrition.csv\"\n",
    "\n",
    "# Find the section with the detailed nutrition facts\n",
    "section = soup.find('section', id='detailed-nutrition-facts')\n",
    "if not section:\n",
    "    raise ValueError(\"Could not find the section with id='detailed-nutrition-facts'\")\n",
    "\n",
    "# Find the table with nutrition data\n",
    "table = section.find('table', id='detailedNutritionDataTable')\n",
    "if not table:\n",
    "    raise ValueError(\"Could not find the table with id='detailedNutritionDataTable'\")\n",
    "\n",
    "# Extract all table rows (tr elements)\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract table data into a list\n",
    "data = []\n",
    "current_category = None\n",
    "for row in rows:\n",
    "    header = row.find('th', class_='headerRow', colspan='3')\n",
    "    if header:\n",
    "        current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "    cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "    if cols:\n",
    "        data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "# Remove rows where the first and second column values are the same\n",
    "drop_indices = []\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i, 1] == df.iloc[i, 2]:  # Compare \"Name\" and \"Measurement\"\n",
    "        drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "    elif df.iloc[i, 1] == \"-- = missing data\":\n",
    "        drop_indices.extend([i])\n",
    "\n",
    "df.drop(index=drop_indices, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove duplicate names in the \"Name\" column\n",
    "df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Handle subcategories\n",
    "filtered_data = []\n",
    "current_category = None\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):  # Detect subcategory\n",
    "        current_category = row[\"Name\"]  # Update current subcategory\n",
    "    else:\n",
    "        filtered_data.append([current_category] + row[1:].tolist())  # Assign subcategory\n",
    "\n",
    "# Create updated DataFrame\n",
    "df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"File saved: {file_path}\")\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()"
   ],
   "id": "113449adbd2b5222",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Categories             Name Measurement Daily Value\n",
      "0               Macronutrients           Weight        100g            \n",
      "1               Macronutrients  Calories (kcal)         585         29%\n",
      "2               Macronutrients              Fat       60.4g         77%\n",
      "3               Macronutrients          Protein       10.1g         20%\n",
      "4               Macronutrients     Carbohydrate       0.32g          0%\n",
      "..                         ...              ...         ...         ...\n",
      "151  Non-Essential Amino Acids    Aspartic acid       984mg            \n",
      "152  Non-Essential Amino Acids          Betaine         2mg            \n",
      "153  Non-Essential Amino Acids    Glutamic acid      1605mg            \n",
      "154  Non-Essential Amino Acids   Hydroxyproline        26mg            \n",
      "155  Non-Essential Amino Acids           Serine       435mg            \n",
      "\n",
      "[156 rows x 4 columns]\n",
      "File saved: C:/Users/laure/OneDrive/Documents/GitHub/KG_project/Pork_fresh_composite_of_separable_fat_with_added_solution_cooked_nutrition.csv\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set up options for headless Chrome\n",
    "options = Options()\n",
    "options.headless = True  # Enable headless mode for invisible operation\n",
    "options.add_argument(\"--window-size=1920,1200\")  # Define the window size of the browser\n",
    "\n",
    "# Set the path to the Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "\n",
    "# Create a Service instance with the path to ChromeDriver\n",
    "service = Service(DRIVER_PATH)\n",
    "\n",
    "# Initialize Chrome with the specified options\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Loop over the range of page IDs (4539 to 749420)\n",
    "for page_id in range(187741, 749420):\n",
    "    # Build the URL for each page\n",
    "    url = f\"https://tools.myfooddata.com/nutrition-facts/{page_id}/wt2/1\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Check if the page exists (i.e., doesn't show the \"foodNotFound\" message)\n",
    "    food_not_found_tag = soup.find('h1', id='foodNotFound')\n",
    "    if food_not_found_tag and food_not_found_tag.get_text(strip=True) == \"Sorry, we can't find that food. Please use the search box to search for another one.\":\n",
    "        print(f\"Food not found for ID: {page_id}, skipping...\")\n",
    "        time.sleep(1)\n",
    "        continue  # Skip to the next iteration if food is not found\n",
    "\n",
    "    # Extract food name from the <h1 class=\"nutritionFactsTitle\"> tag\n",
    "    food_name_tag = soup.find('h1', class_='nutritionFactsTitle')\n",
    "    print(food_name_tag.get_text(strip=True))\n",
    "    if food_name_tag.get_text(strip=True)!=\"\":\n",
    "        food_name = food_name_tag.get_text(strip=True)\n",
    "    else:\n",
    "        print(f\"Could not find food name for ID: {page_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Clean the food name to make it safe for a file path (remove special characters and spaces)\n",
    "    food_name = re.sub(r',', '', food_name)\n",
    "    safe_food_name = re.sub(r'[^a-zA-Z0-9_\\-]', '_', food_name)\n",
    "\n",
    "    # Set the file path based on the food name\n",
    "    file_path = f\"C:/Users/laure/OneDrive/Documents/GitHub/KG_project/Nutrition/{safe_food_name + '_' + str(page_id)}.csv\"\n",
    "\n",
    "    # Find the section with the detailed nutrition facts\n",
    "    section = soup.find('section', id='detailed-nutrition-facts')\n",
    "    if not section:\n",
    "        print(f\"Could not find the nutrition facts section for ID: {page_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Find the table with nutrition data\n",
    "    table = section.find('table', id='detailedNutritionDataTable')\n",
    "    if not table:\n",
    "        print(f\"Could not find the nutrition table for ID: {page_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Extract all table rows (tr elements)\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Extract table data into a list\n",
    "    data = []\n",
    "    current_category = None\n",
    "    for row in rows:\n",
    "        header = row.find('th', class_='headerRow', colspan='3')\n",
    "        if header:\n",
    "            current_category = header.get_text(strip=True)  # Update category if header row is found\n",
    "        cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "        if cols:\n",
    "            data.append([current_category] + cols)  # Prepend category to row data\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "    # Remove rows where the first and second column values are the same\n",
    "    drop_indices = []\n",
    "    for i in range(len(df) - 1):\n",
    "        if df.iloc[i, 1] == df.iloc[i, 2]:  # Compare \"Name\" and \"Measurement\"\n",
    "            drop_indices.extend([i, i + 1])  # Mark row and the row below it for removal\n",
    "        elif df.iloc[i, 1] == \"-- = missing data\":\n",
    "            drop_indices.extend([i])\n",
    "\n",
    "    df.drop(index=drop_indices, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Remove duplicate names in the \"Name\" column\n",
    "    df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Handle subcategories\n",
    "    filtered_data = []\n",
    "    current_category = None\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):  # Detect subcategory\n",
    "            current_category = row[\"Name\"]  # Update current subcategory\n",
    "        else:\n",
    "            filtered_data.append([current_category] + row[1:].tolist())  # Assign subcategory\n",
    "\n",
    "    # Create updated DataFrame\n",
    "    df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "    print(f\"Saving data for food: {food_name} (ID: {page_id})\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    #print(f\"File saved: {file_path}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Close the Selenium driver\n",
    "driver.quit()"
   ],
   "id": "50316b682098b2fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final version that was used for the webscrapping",
   "id": "3a1d5e9ac136491e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Set up Selenium options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Set the path to Chromedriver\n",
    "DRIVER_PATH = 'C:/Users/laure/OneDrive/Documents/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(DRIVER_PATH)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "start_id = 354157\n",
    "#end_id = 749420\n",
    "end_id = 400000\n",
    "step = 50  # Initial step size\n",
    "one_by_one_count = 0  # Counter for consecutive foodNotFound in one-by-one mode\n",
    "\n",
    "page_id = start_id\n",
    "\n",
    "while page_id < end_id:\n",
    "    url = f\"https://tools.myfooddata.com/nutrition-facts/{page_id}/wt2/1\"\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    food_not_found_tag = soup.find('h1', id='foodNotFound')\n",
    "\n",
    "    if food_not_found_tag:\n",
    "        print(f\"Food not found for ID: {page_id}, skipping...\")\n",
    "\n",
    "        # Continue one-by-one; if stuck in foodNotFound for 100 iterations, return to step 100\n",
    "        one_by_one_count += 1\n",
    "        if one_by_one_count > 50:\n",
    "            step = 50  # Resume batch mode\n",
    "            one_by_one_count = 0\n",
    "\n",
    "        page_id += step\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "    # Extract food name\n",
    "    food_name_tag = soup.find('h1', class_='nutritionFactsTitle')\n",
    "    if not food_name_tag or not food_name_tag.get_text(strip=True):\n",
    "        print(f\"Could not find food name for ID: {page_id}, skipping...\")\n",
    "        one_by_one_count += 1\n",
    "        if one_by_one_count > 50:\n",
    "            step = 50  # Resume batch mode\n",
    "            one_by_one_count = 0\n",
    "        page_id += step\n",
    "        continue\n",
    "\n",
    "    food_name = re.sub(r',', '', food_name_tag.get_text(strip=True))\n",
    "    safe_food_name = re.sub(r'[^a-zA-Z0-9_\\-]', '_', food_name)\n",
    "    safe_food_name = safe_food_name[:150]\n",
    "    file_path = f\"C:/Users/laure/OneDrive/Documents/GitHub/Dataset_KG/Nutrition/{safe_food_name}_{page_id}.csv\"\n",
    "\n",
    "    # Find the nutrition table\n",
    "    section = soup.find('section', id='detailed-nutrition-facts')\n",
    "    table = section.find('table', id='detailedNutritionDataTable') if section else None\n",
    "    if not table:\n",
    "        print(f\"Could not find nutrition data for ID: {page_id}, skipping...\")\n",
    "        one_by_one_count += 1\n",
    "        if one_by_one_count > 50:\n",
    "            step = 50  # Resume batch mode\n",
    "            one_by_one_count = 0\n",
    "        page_id += step\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Extract table data\n",
    "    rows = table.find_all('tr')\n",
    "    data = []\n",
    "    current_category = None\n",
    "    for row in rows:\n",
    "        header = row.find('th', class_='headerRow', colspan='3')\n",
    "        if header:\n",
    "            current_category = header.get_text(strip=True)\n",
    "        cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "        if cols:\n",
    "            data.append([current_category] + cols)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "\n",
    "    # Data cleaning: remove duplicates and missing values\n",
    "    drop_indices = [i for i in range(len(df) - 1) if df.iloc[i, 1] == df.iloc[i, 2] or df.iloc[i, 1] == \"-- = missing data\"]\n",
    "    df.drop(index=drop_indices, inplace=True)\n",
    "    df.drop_duplicates(subset=[\"Name\"], keep=\"first\", inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Handle subcategories\n",
    "    filtered_data = []\n",
    "    current_category = None\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[\"Measurement\"]) and pd.isna(row[\"Daily Value\"]):\n",
    "            current_category = row[\"Name\"]\n",
    "        else:\n",
    "            filtered_data.append([current_category] + row[1:].tolist())\n",
    "\n",
    "    df = pd.DataFrame(filtered_data, columns=[\"Categories\", \"Name\", \"Measurement\", \"Daily Value\"])\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Saved food: {food_name} (ID: {page_id})\")\n",
    "    if step == 50:\n",
    "        # Switch to one-by-one mode\n",
    "        page_id -= 50  # Go back\n",
    "        step = 1  # Start one-by-one search\n",
    "        one_by_one_count = 0\n",
    "    else:\n",
    "        page_id += step\n",
    "        one_by_one_count = 0\n",
    "    time.sleep(1)\n",
    "\n",
    "driver.quit()"
   ],
   "id": "f923e1d06928bea9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
